{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file needs to be run throughout for every airline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing project dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz#egg=en_core_web_sm==2.3.0 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from en_core_web_sm==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (4.47.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (49.2.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (0.7.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.19.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.21.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.7.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kanishk verma\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.1.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Kanishk Verma\\Anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Kanishk\n",
      "[nltk_data]     Verma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Kanishk\n",
      "[nltk_data]     Verma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = list(stopwords.words('english'))\n",
    "newStop = ['was', 'and', 'the', 'to', 'in', 'of', 'a', 'an', 'is', 'were', 'for', 'with', 'are', 'one', 'our', 'gave', 'have',\n",
    "           'me', 'an', 'i', 'or', 'had', 'did', 'get', 'made', 'take', 'given', 'told', 'let', 'us']\n",
    "stopWords = stopWords+newStop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Stanford's CoreNLP Dependency Parsing and other Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_parser = nltk.parse.CoreNLPDependencyParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = {'Entertainment':'e', 'Bookings':'o', 'Cabin':'c',\n",
    "        'Food':'f', 'In-flight service':'i', \n",
    "          'Off-flight service':'o', 'Possession':'p', \n",
    "        'Seat':'s', 'Staff':'st'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Path and Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change for both airlines\n",
    "path_json1_ta = r'C:\\Users\\Kanishk Verma\\Desktop\\Practicum\\preprocess\\ToAnnotate\\TripAdvisorAnnotated\\json1'\n",
    "path_meta_ta = r'C:\\Users\\Kanishk Verma\\Desktop\\Practicum\\preprocess\\ToAnnotate\\TripAdvisorAnnotated\\metadata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_json1 = glob.glob(path_json1_ta+r'\\*.json1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_meta = glob.glob(path_meta_ta+r'\\*_a.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_json1), len(file_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFiles(file):\n",
    "    '''\n",
    "    This function load's the metadata and annotated file generated after labelling\n",
    "    '''\n",
    "    infoAnn = []\n",
    "    infoMeta = []\n",
    "    for i in range(len(file)):\n",
    "        if (file[i].endswith('.json1')):\n",
    "            jsonDic = {}\n",
    "            airline = file[i].split('\\\\')[-1].split(\".\")[0].split('Annotation')[0].lower() \n",
    "            info = []\n",
    "            for line in open(file[i], encoding='utf-8'):\n",
    "                info.append(json.loads(line))\n",
    "            jsonDic['Airline'] = airline\n",
    "            jsonDic['Info'] = info\n",
    "            print(\"Annotation File Loaded for Airline: \"+airline)\n",
    "            infoAnn.append(jsonDic)\n",
    "        else:\n",
    "            jsonDic = {}\n",
    "            airline = file[i].split('\\\\')[-1].split(\".\")[0].split('_a')[0].lower()\n",
    "            info = []\n",
    "            for line in open(file[i], encoding='utf-8'):\n",
    "                info.append(json.loads(line))\n",
    "            jsonDic['Airline'] = airline\n",
    "            jsonDic['Info'] = info\n",
    "            print(\"Metadata File Loaded for Airline: \"+airline)\n",
    "            infoMeta.append(jsonDic)\n",
    "    if (infoAnn != [] and infoMeta == []):\n",
    "        return infoAnn\n",
    "    elif (infoMeta != [] and infoAnn == []):\n",
    "        return infoMeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Annotation Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation File Loaded for Airline: arabia\n",
      "Annotation File Loaded for Airline: asia\n",
      "Annotation File Loaded for Airline: british\n",
      "Annotation File Loaded for Airline: canada\n",
      "Annotation File Loaded for Airline: emirates\n",
      "Annotation File Loaded for Airline: france\n",
      "Annotation File Loaded for Airline: india\n",
      "Annotation File Loaded for Airline: united\n",
      "Annotation File Loaded for Airline: virginatlantic\n",
      "Annotation File Loaded for Airline: virginaus\n",
      "Annotation File Loaded for Airline: vistara\n"
     ]
    }
   ],
   "source": [
    "infoAnn = loadFiles(file_json1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Loaded for Airline: arabia\n",
      "Metadata File Loaded for Airline: asia\n",
      "Metadata File Loaded for Airline: british\n",
      "Metadata File Loaded for Airline: canada\n",
      "Metadata File Loaded for Airline: emirates\n",
      "Metadata File Loaded for Airline: france\n",
      "Metadata File Loaded for Airline: india\n",
      "Metadata File Loaded for Airline: united\n",
      "Metadata File Loaded for Airline: virginatlantic\n",
      "Metadata File Loaded for Airline: virginaus\n",
      "Metadata File Loaded for Airline: vistara\n"
     ]
    }
   ],
   "source": [
    "infoMeta = loadFiles(file_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Airline Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlineNames = []\n",
    "for i in range(len(infoAnn)):\n",
    "    airlineNames.append(infoAnn[i]['Airline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(airlineNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabia',\n",
       " 'asia',\n",
       " 'british',\n",
       " 'canada',\n",
       " 'emirates',\n",
       " 'france',\n",
       " 'india',\n",
       " 'united',\n",
       " 'virginatlantic',\n",
       " 'virginaus',\n",
       " 'vistara']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlineNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting number of reviews in each airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airline: arabia has 96 reviews\n",
      "Airline: asia has 99 reviews\n",
      "Airline: british has 101 reviews\n",
      "Airline: canada has 99 reviews\n",
      "Airline: emirates has 100 reviews\n",
      "Airline: france has 99 reviews\n",
      "Airline: india has 19 reviews\n",
      "Airline: united has 59 reviews\n",
      "Airline: virginatlantic has 82 reviews\n",
      "Airline: virginaus has 78 reviews\n",
      "Airline: vistara has 68 reviews\n",
      "Total Review:900\n"
     ]
    }
   ],
   "source": [
    "sums = 0\n",
    "for i in range(len(infoAnn)):\n",
    "    for airline in airlineNames:\n",
    "        if (infoAnn[i]['Airline'] == airline):\n",
    "            ann = infoAnn[i]['Info']\n",
    "            c = 0\n",
    "            for j in range(len(ann)):\n",
    "                if (ann[j]['labels'] != []):\n",
    "                    c += 1\n",
    "            print(\"Airline: \"+airline+\" has \"+str(c)+\" reviews\")\n",
    "            sums = sums+c\n",
    "print(\"Total Review:\"+str(sums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering detailed information airline-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(infoAnn)):\n",
    "    for j in range(len(infoMeta)):\n",
    "        if (infoAnn[i]['Airline']=='vistara'): # Change Airline Name here\n",
    "            ann = infoAnn[i]['Info']\n",
    "        if (infoMeta[j]['Airline']=='vistara'): # Change Airline Name here\n",
    "            meta = infoMeta[j]['Info'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(716, 716)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ann), len(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "annNew = []\n",
    "for i in range(len(ann)):\n",
    "    if (ann[i-1]['text'] != ann[i]['text']):\n",
    "        if(ann[i]['labels'] != []):\n",
    "            annNew.append(ann[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaNew = []\n",
    "for i in range(len(meta)):\n",
    "    for j in range(len(annNew)):\n",
    "        if(meta[i]['id'] == annNew[j]['id']):\n",
    "            metaNew.append(meta[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metaNew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The detailed information generated by Doccano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 3,\n",
       " 'text': 'the flight was on time and the food was very goodneat and cleanthis was my first travel with vistaraenjoyed it but the leg space is very less in the economy classwhile travelling back from port blair we had a premium economy ticket and enjoyed the luxury with excellent food and beverages and much more leg spaces',\n",
       " 'meta': {},\n",
       " 'annotation_approver': None,\n",
       " 'labels': [[4, 22, 'Off-flight-service'],\n",
       "  [119, 141, 'Cabin'],\n",
       "  [260, 288, 'Food'],\n",
       "  [31, 49, 'Food'],\n",
       "  [49, 63, 'Cabin'],\n",
       "  [298, 313, 'Cabin'],\n",
       "  [236, 244, 'Positive Opinion'],\n",
       "  [100, 107, 'Positive Opinion']]}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annNew[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 3,\n",
       " 'Entertainment': [],\n",
       " 'Bookings': [],\n",
       " 'Cabin': ['leg space is very less', 'neat and clean', 'more leg spaces'],\n",
       " 'Food': ['excellent food and beverages', 'food was very good'],\n",
       " 'In-flight service': [],\n",
       " 'Off-flight service': ['flight was on time'],\n",
       " 'Positive Opinion': ['enjoyed', 'enjoyed'],\n",
       " 'Negative Opinion': [],\n",
       " 'Neutral Opinion': [],\n",
       " 'Possesion': [],\n",
       " 'Seat': [],\n",
       " 'Staff': []}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaNew[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Entertainment', 'Bookings', 'Cabin', 'Food',\n",
    "'In-flight service', 'Off-flight service', \n",
    "'Positive Opinion', 'Negative Opinion', 'Neutral Opinion',\n",
    "'Possession', 'Seat','Staff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetails(ann, meta):\n",
    "    '''\n",
    "    This function captures details like id, text, sentences and labels for each review\n",
    "    '''\n",
    "    finLs = []\n",
    "    for i in range(len(ann)):\n",
    "        for j in range(len(meta)):\n",
    "            nDic = {}\n",
    "            if ann[i]['id'] == meta[j]['id']:\n",
    "                ids = meta[j]['id']\n",
    "                txt = ann[i]['text']\n",
    "                nDic['id'] = ids\n",
    "                nDic['text'] = txt\n",
    "                doc = nlp(txt)\n",
    "                sentence = []\n",
    "                for sent in doc.sents:\n",
    "                    sentence.append(sent)\n",
    "                nDic['sentence'] = sentence\n",
    "                ls = []\n",
    "                for k, v in meta[j].items():\n",
    "                    lb = []\n",
    "                    dic = {}\n",
    "                    if k in labels and v != []:\n",
    "                        for l in range(len(v)):\n",
    "                            lb.append(v[l])\n",
    "                        dic[k] = lb\n",
    "                        ls.append(dic)\n",
    "                nDic['Labels'] = ls\n",
    "                finLs.append(nDic)\n",
    "    return finLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "finLs = getDetails(annNew, metaNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVal(sentTkn, lab, ab):\n",
    "    '''\n",
    "    This function captures dependency parsing in the form of governor (head)  and dependent word relation\n",
    "    '''\n",
    "    \n",
    "    ls = []\n",
    "    for i in range(len(sentTkn)):\n",
    "        for j in range(len(lab)):\n",
    "            for k, v in lab[j].items():\n",
    "                for l in range(len(v)):\n",
    "                    lbtkn = v[l].split()\n",
    "                    for m in range(len(lbtkn)):\n",
    "                        if (lbtkn[m] == sentTkn[i]):\n",
    "                            for o in range(len(ab)):\n",
    "                                for lbl, val in lbls.items():\n",
    "                                    if (k == lbl):\n",
    "                                        if (ab[o]['governorGloss'] == lbtkn[m] and ab[o]['dependentGloss'] not in stopWords):\n",
    "                                            if (i == 0):\n",
    "                                                a = (val, lbtkn[m], ab[o]['dependentGloss'], ab[o]['dep'])\n",
    "                                            elif (i == len(sentTkn)-1):\n",
    "                                                a = (val, lbtkn[m], ab[o]['dependentGloss'], ab[o]['dep'])\n",
    "                                            else:\n",
    "                                                a = (val, lbtkn[m], ab[o]['dependentGloss'], ab[o]['dep'])\n",
    "                                        elif (ab[o]['dependentGloss'] == lbtkn[m] and ab[o]['governorGloss'] not in stopWords and ab[o]['governorGloss'] != 'ROOT'):\n",
    "                                            if (i == 0):\n",
    "                                                a = (val, ab[o]['governorGloss'], lbtkn[m], ab[o]['dep'])\n",
    "                                            elif (i == len(sentTkn)-1):\n",
    "                                                a = (val, ab[o]['governorGloss'], lbtkn[m], ab[o]['dep'])\n",
    "                                            else:\n",
    "                                                a = (val, ab[o]['governorGloss'], lbtkn[m], ab[o]['dep'])\n",
    "                                            ls.append(a)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDep(finLs):\n",
    "    '''\n",
    "    Ths function captures Part-of-speech tag, dependency realtion and dependency tag of all labelled words in a review\n",
    "    '''\n",
    "    nnLs = []\n",
    "    for i in range(len(finLs)):\n",
    "        d = finLs[i]\n",
    "        sents = d['sentence']\n",
    "        lab = d['Labels']\n",
    "        dic = {}\n",
    "        dic['id'] = d['id']\n",
    "        dic['sentence'] = sents\n",
    "        dic['labels'] = lab\n",
    "        nls = []\n",
    "        print(\"Running for Review: \"+str(i))\n",
    "        for j in range(len(sents)):\n",
    "            try:\n",
    "                sents[j] = str(sents[j])\n",
    "                dictval = {}\n",
    "                sentTkn = str(sents[j]).split()\n",
    "                output = nnlp.annotate(str(sents[j]),\n",
    "                              properties={\n",
    "                      'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
    "                      'outputFormat': 'json'\n",
    "                      })\n",
    "        \n",
    "                dictval[j] = getVal(sentTkn, lab, ab)\n",
    "                nls.append(dictval)\n",
    "            except:\n",
    "                sents[j] = str(sents[j])\n",
    "                dictval = {}\n",
    "                s = str(sents[j]).split()\n",
    "                ns = []\n",
    "                for k in range(len(s)):\n",
    "                    if (k%5 == 0 and (k<len(s)-1)):\n",
    "                        if (len(s)-k == 4):\n",
    "                            ns.append(s[k]+' '+s[k+1]+' '+s[k+2]+' '+s[k+3])#+' '+s[k+4])\n",
    "                        elif (len(s)-k == 3):\n",
    "                            ns.append(s[k]+' '+s[k+1]+' '+s[k+2])#+' '+s[k+3])\n",
    "                        elif (len(s)-k == 2):\n",
    "                            ns.append(s[k]+' '+s[k+1])\n",
    "                        elif (len(s)-k == 1):\n",
    "                            ns.append(s[k])\n",
    "                        \n",
    "                for l in range(len(ns)):\n",
    "                    sentTkn = ns[l].split()\n",
    "                    output = nnlp.annotate(str(sents[j]),\n",
    "                              properties={\n",
    "                      'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
    "                      'outputFormat': 'json'\n",
    "                      })\n",
    "                    if (isinstance(output, str) == False):\n",
    "                        ab = output['sentences'][0]['enhancedDependencies']\n",
    "                        dictval[j] = getVal(sentTkn, lab, ab)\n",
    "                        nls.append(dictval)\n",
    "                    else:\n",
    "                        continue;\n",
    "        dic['Values'] = nls\n",
    "        nnLs.append(dic)\n",
    "    return nnLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing dependency and pos tags for all review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Review: 0\n",
      "Running for Review: 1\n",
      "Running for Review: 2\n",
      "Running for Review: 3\n",
      "Running for Review: 4\n",
      "Running for Review: 5\n",
      "Running for Review: 6\n",
      "Running for Review: 7\n",
      "Running for Review: 8\n",
      "Running for Review: 9\n",
      "Running for Review: 10\n",
      "Running for Review: 11\n",
      "Running for Review: 12\n",
      "Running for Review: 13\n",
      "Running for Review: 14\n",
      "Running for Review: 15\n",
      "Running for Review: 16\n",
      "Running for Review: 17\n",
      "Running for Review: 18\n",
      "Running for Review: 19\n",
      "Running for Review: 20\n",
      "Running for Review: 21\n",
      "Running for Review: 22\n",
      "Running for Review: 23\n",
      "Running for Review: 24\n",
      "Running for Review: 25\n",
      "Running for Review: 26\n",
      "Running for Review: 27\n",
      "Running for Review: 28\n",
      "Running for Review: 29\n",
      "Running for Review: 30\n",
      "Running for Review: 31\n",
      "Running for Review: 32\n",
      "Running for Review: 33\n",
      "Running for Review: 34\n",
      "Running for Review: 35\n",
      "Running for Review: 36\n",
      "Running for Review: 37\n",
      "Running for Review: 38\n",
      "Running for Review: 39\n",
      "Running for Review: 40\n",
      "Running for Review: 41\n",
      "Running for Review: 42\n",
      "Running for Review: 43\n",
      "Running for Review: 44\n",
      "Running for Review: 45\n",
      "Running for Review: 46\n",
      "Running for Review: 47\n",
      "Running for Review: 48\n",
      "Running for Review: 49\n",
      "Running for Review: 50\n",
      "Running for Review: 51\n",
      "Running for Review: 52\n",
      "Running for Review: 53\n",
      "Running for Review: 54\n",
      "Running for Review: 55\n",
      "Running for Review: 56\n",
      "Running for Review: 57\n",
      "Running for Review: 58\n",
      "Running for Review: 59\n",
      "Running for Review: 60\n",
      "Running for Review: 61\n",
      "Running for Review: 62\n",
      "Running for Review: 63\n",
      "Running for Review: 64\n",
      "Running for Review: 65\n"
     ]
    }
   ],
   "source": [
    "nnLs = getDep(finLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nnLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'sentence': ['the flights good space is better then indigo and on time the snacks offered are a bit average in terms of variety and can improve the staff is mature well trained and pleasant  food was served on good time and was decent in quality'],\n",
       " 'labels': [{'Cabin': ['good space']},\n",
       "  {'Food': ['on time the snacks',\n",
       "    'decent in quality',\n",
       "    'pleasant  food',\n",
       "    'average in terms of variety',\n",
       "    'served on good time']},\n",
       "  {'Staff': ['staff is mature', 'well trained']}],\n",
       " 'Values': [{0: [('f', 'served', 'decent', 'conj:and'),\n",
       "    ('f', 'terms', 'in', 'case'),\n",
       "    ('f', 'quality', 'in', 'case'),\n",
       "    ('f', 'terms', 'in', 'case'),\n",
       "    ('f', 'quality', 'in', 'case'),\n",
       "    ('f', 'decent', 'quality', 'obl:in')]}]}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnLs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsPosDep = []\n",
    "for i in range(len(nnLs)):\n",
    "    dicPos = {}\n",
    "    ids = nnLs[i]['id']\n",
    "    lab = nnLs[i]['labels']\n",
    "    vals = nnLs[i]['Values']\n",
    "    sent = nnLs[i]['sentence']\n",
    "    dicPos['id'] = ids\n",
    "    dicPos['sentence'] = sent\n",
    "    dicPos['labels'] = lab\n",
    "    tmpls = []\n",
    "    \n",
    "    for j in range(len(vals)):\n",
    "        for k,v in vals[j].items():\n",
    "            for l in range(len(sent)):\n",
    "                if (v!=[] and k==l):\n",
    "                    par, = dep_parser.raw_parse(sent[l])\n",
    "                    abx = par.nodes\n",
    "                    for o in range(len(lab)):\n",
    "                        for p,q in lab[o].items():\n",
    "                            for s in range(len(q)):\n",
    "                                lbtkn = q[s].split()\n",
    "                            \n",
    "                                for r in range(len(lbtkn)):\n",
    "                                    for m in range(len(v)):\n",
    "                                        \n",
    "                                        tempdic = {}\n",
    "                                        for key, value in abx.items():\n",
    "                                            abxls = []\n",
    "                                            if(v[m][1] == lbtkn[r] and lbtkn[r] == abx[key]['word']):\n",
    "                                            \n",
    "                                                rg = ((v[m][1], abx[key]['tag']), v[m][2], v[m][-1], v[m][0])\n",
    "                                                #abxls.append(rg)\n",
    "                                            elif(v[m][2] == lbtkn[r] and lbtkn[r] == abx[key]['word']):\n",
    "                                            \n",
    "                                                rg = (v[m][1], (v[m][2], abx[key]['tag']), v[m][-1], v[m][0])\n",
    "                                            \n",
    "                                            abxls.append(rg)\n",
    "                                        tempdic[l] = list(set(abxls))\n",
    "                                        tmpls.append(tempdic)\n",
    "    cmn = []\n",
    "    td = {}\n",
    "    for i in range(len(tmpls)):\n",
    "        for k,v in tmpls[i].items():\n",
    "            for ke, va in tmpls[i-1].items():\n",
    "                if (k==ke):\n",
    "                    if(v==va):\n",
    "                        cmn.append(va[0])\n",
    "    cmn = list(set(cmn))\n",
    "    dicPos['Values'] = cmn\n",
    "    \n",
    "    lsPosDep.append(dicPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lsPosDep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'sentence': ['the flights good space is better then indigo and on time the snacks offered are a bit average in terms of variety and can improve the staff is mature well trained and pleasant  food was served on good time and was decent in quality'],\n",
       " 'labels': [{'Cabin': ['good space']},\n",
       "  {'Food': ['on time the snacks',\n",
       "    'decent in quality',\n",
       "    'pleasant  food',\n",
       "    'average in terms of variety',\n",
       "    'served on good time']},\n",
       "  {'Staff': ['staff is mature', 'well trained']}],\n",
       " 'Values': [('served', ('decent', 'JJ'), 'conj:and', 'f'),\n",
       "  ('quality', ('in', 'IN'), 'case', 'f'),\n",
       "  (('quality', 'NN'), 'in', 'case', 'f'),\n",
       "  (('decent', 'JJ'), 'quality', 'obl:in', 'f'),\n",
       "  ('flight', ('was', 'VBD'), 'cop', 'o'),\n",
       "  (('terms', 'NNS'), 'in', 'case', 'f'),\n",
       "  ('decent', ('quality', 'NN'), 'obl:in', 'f'),\n",
       "  (('served', 'VBN'), 'decent', 'conj:and', 'f')]}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsPosDep[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capturing Sentiment score for the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for Review:1\n",
      "Processing for Review:2\n",
      "Processing for Review:3\n",
      "Processing for Review:4\n",
      "Processing for Review:5\n",
      "Processing for Review:6\n",
      "Processing for Review:7\n",
      "Processing for Review:8\n",
      "Processing for Review:9\n",
      "Processing for Review:10\n",
      "Processing for Review:11\n",
      "Processing for Review:12\n",
      "Processing for Review:13\n",
      "Processing for Review:14\n",
      "Processing for Review:15\n",
      "Processing for Review:16\n",
      "Processing for Review:17\n",
      "Processing for Review:18\n",
      "Processing for Review:19\n",
      "Processing for Review:20\n",
      "Processing for Review:21\n",
      "Processing for Review:22\n",
      "Processing for Review:23\n",
      "Processing for Review:24\n",
      "Processing for Review:25\n",
      "Processing for Review:26\n",
      "Processing for Review:27\n",
      "Processing for Review:28\n",
      "Processing for Review:29\n",
      "Processing for Review:30\n",
      "Processing for Review:31\n",
      "Processing for Review:32\n",
      "Processing for Review:33\n",
      "Processing for Review:34\n",
      "Processing for Review:35\n",
      "Processing for Review:36\n",
      "Processing for Review:37\n",
      "Processing for Review:38\n",
      "Processing for Review:39\n",
      "Processing for Review:40\n",
      "Processing for Review:41\n",
      "Processing for Review:42\n",
      "Processing for Review:43\n",
      "Processing for Review:44\n",
      "Processing for Review:45\n",
      "Processing for Review:46\n",
      "Processing for Review:47\n",
      "Processing for Review:48\n",
      "Processing for Review:49\n",
      "Processing for Review:50\n",
      "Processing for Review:51\n",
      "Processing for Review:52\n",
      "Processing for Review:53\n",
      "Processing for Review:54\n",
      "Processing for Review:55\n",
      "Processing for Review:56\n",
      "Processing for Review:57\n",
      "Processing for Review:58\n",
      "Processing for Review:59\n",
      "Processing for Review:60\n",
      "Processing for Review:61\n",
      "Processing for Review:62\n",
      "Processing for Review:63\n",
      "Processing for Review:64\n",
      "Processing for Review:65\n",
      "Processing for Review:66\n",
      "****Finito****\n"
     ]
    }
   ],
   "source": [
    "lsPosSentDep = []\n",
    "for h in range(len(lsPosDep)):\n",
    "    newDic = {}\n",
    "    values = lsPosDep[h]['Values']\n",
    "    sent = lsPosDep[h]['sentence']\n",
    "    ids = lsPosDep[h]['id']\n",
    "    nval = []\n",
    "    print(\"Processing for Review:\" +str(h+1))\n",
    "    for i in range(len(values)):\n",
    "        for j in range(len(sent)):\n",
    "            par, = dep_parser.raw_parse(sent[j])\n",
    "            a = par.nodes\n",
    "            wTkn = nltk.word_tokenize(sent[j])\n",
    "            for word in wTkn:\n",
    "                for k,v in a.items():\n",
    "                    if (isinstance(values[i][0], tuple) == True):\n",
    "                        if (values[i][1] == a[k]['word']):\n",
    "                            if (values[i][0][0] == word):\n",
    "                                    pol = sid.polarity_scores(word)['compound']\n",
    "                                    pol = round(pol,2)\n",
    "                                ax = (0, (values[i][0][0],values[i][0][1],pol), (a[k]['word'], a[k]['tag']), values[i][-2], values[i][-1])\n",
    "                    \n",
    "                    elif (isinstance(values[i][1], tuple) == True):\n",
    "                        if (values[i][0] == a[k]['word']):\n",
    "                            if (values[i][1][0] == word):\n",
    "                                pol = sid.polarity_scores(word)['compound']\n",
    "                                pol = round(pol,2)\n",
    "                                ax = (1, (a[k]['word'], a[k]['tag']), (values[i][1][0],values[i][1][1], pol), values[i][-2], values[i][-1])\n",
    "                    \n",
    "                    nval.append(ax)\n",
    "                nval = list(set(nval))\n",
    "    newDic['id'] = ids\n",
    "    newDic['sentence'] = sent\n",
    "    newDic['Details'] = nval\n",
    "    newDic['Labels'] = lsPosDep[h]['labels']\n",
    "    lsPosSentDep.append(newDic)\n",
    "print(\"****Finito****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lsPosSentDep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'sentence': ['the flights good space is better then indigo and on time the snacks offered are a bit average in terms of variety and can improve the staff is mature well trained and pleasant  food was served on good time and was decent in quality'],\n",
       " 'Details': [(0, ('quality', 'NN', 0.0), ('in', 'IN'), 'case', 'f'),\n",
       "  (1, ('quality', 'NN'), ('in', 'IN', 0.0), 'case', 'f'),\n",
       "  (0, ('decent', 'JJ', 0.0), ('quality', 'NN'), 'obl:in', 'f'),\n",
       "  (1, ('decent', 'JJ'), ('quality', 'NN', 0.0), 'obl:in', 'f'),\n",
       "  (0, ('served', 'VBN', 0.0), ('decent', 'JJ'), 'conj:and', 'f'),\n",
       "  (1, ('flight', 'NN'), ('was', 'VBD', 0.0), 'cop', 'f'),\n",
       "  (0, ('terms', 'NNS', 0.0), ('in', 'IN'), 'case', 'f'),\n",
       "  (1, ('served', 'VBN'), ('decent', 'JJ', 0.0), 'conj:and', 'f')],\n",
       " 'Labels': [{'Cabin': ['good space']},\n",
       "  {'Food': ['on time the snacks',\n",
       "    'decent in quality',\n",
       "    'pleasant  food',\n",
       "    'average in terms of variety',\n",
       "    'served on good time']},\n",
       "  {'Staff': ['staff is mature', 'well trained']}]}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsPosSentDep[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving airline wise dependency tag and other features in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = r'C:\\Users\\Kanishk Verma\\Desktop\\Practicum\\DepTag\\TripAdvisor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(savePath+r'\\Vistara_TA'+'DepTag.json', 'w') as outfile:\n",
    "    json.dump(lsPosSentDep, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
